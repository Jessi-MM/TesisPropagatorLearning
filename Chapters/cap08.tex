%************************************************
\chapter{LSTM como propagador en Dinámica Cuántica}\label{ch:LSTMapplied}
% ************************************************
\section{Introducción}
En el capítulo anterior se desarrollaron conceptos básicos generales de las redes neuronales, actualmente existen diversos tipos de redes que se aplican a problemas comunes y que se distinguen por su arquitectura.
\\
En este capítulo se abordará un modelo particular de red llamado \emph{Long Short-Term Memory} (\acs{LSTM}, por sus siglas en inglés), que pertenece a un tipo de redes llamadas \emph{Redes Neuronales Recurrentes} (\acp{RNN}, por sus siglas en inglés). Este tipo de redes es comúnmente utilizada cuando los atributos en el conjunto de datos están relacionados entre sí, o guardan una dependencia.
\\\\
En la sección \autoref{sec:lstm} se desarrollará la arquitectura de las redes \acs{LSTM}, así como sus aplicaciones más comunes, haciendo énfasis a las series de tiempo, que son de particular interés en el presente trabajo, pues la evolución temporal de un paquete de onda puede tratarse como un problema de series de tiempo, encontrando así una manera de abordar el problema de propagar una onda en el tiempo utilizando una red \acs{LSTM}.
\\
En la sección \autoref{sec:Project} se implementará el modelo de \acs{LSTM} que servirá como propagador $\mathcal{U}$ en la ecuación de Schrödinger Dependiente del Tiempo (\autoref{eq:TDSE ket}) para los potenciales $V(r,t)$ revisados en la sección \autoref{sec:ProtonTransfer}.
\\
A lo largo de la sección se presentará el conjunto de datos de entrenamiento de la red, así como los parámetros del modelo del potencial que se utilizaron para su obtención; también se presentarán los hiper-parámetros empleados en la red. Finalmente, en la sección \autoref{sec:Resultados} se presentarán los resultados y predicciones obtenidas con del modelo.

\section{Redes Neuronales Recurrentes}\label{sec:RNN}
Como se mencionó anteriormente, las Redes Neuronales Recurrentes se emplean cuando entre los atributos del conjunto de datos existen relaciones. Un dato de entrada para este tipo de redes puede verse de la siguiente forma:

\begin{equation}
  \label{eq:xdataRNN}
  \vec{X} = (\vec{x}_0, \vec{x}_1, \dots , \vec{x}_n)
\end{equation}

en donde cada $\vec{x}_t$ es un vector de dimensión $d$ recibido al tiempo $t$. Para cada tiempo $t$ se tiene un valor de salida: $\vec{y}_t$ con la misma dimensión.
\\
Algunos ejemplos de aplicaciones se muestran a continuación:

\begin{itemize}[label=\textcolor{CTtitle}{\textbullet}]
\item \emph{Secuencias de texto:} En donde la importancia o información de una palabra depende del contexto en el que se encuentra dentro de una oración. Cada palabra\footnote{En este caso, la dimensión $d$ hace referencia al número de palabras que tiene el léxico utilizado.}: $\vec{x}_t$,  puede verse como un atributo relacionado a la palabra anterior en una oración: $\vec{x}_{t-1}$, o a la palabra siguiente: $\vec{x}_{t+1}$.
 \item \emph{Series de tiempo:} $\vec{x}_t$ en este caso puede almacenar $d$ valores, que pueden ser, por ejemplo, los correspondientes al valor de una función $f(x,t)$ en una maya de $d$ puntos en el espacio $x$ al tiempo $t$. En este tipo de configuración la salida $\vec{y}_t$ podría ser la predicción pronosticada de $\vec{x}_{t+1}$.
 \end{itemize}
  
Un diagrama sencillo de una \acs{RNN} se muestra en la \autoref{fig:RNN}, en donde un auto-bucle actualizará el \textbf{estado oculto} $\vec{h}_t$ a cada tiempo $t$ después de la entrada $\vec{x}_t$:
\begin{equation}
  \label{eq:hidden}
  \vec{h}_t = Tanh(W_{xh}\vec{x}_t + W_{hh} \vec{h}_{t-1})
\end{equation}
%\begin{equation}
%  \vec{h}_t = f(\vec{h}_{t-1}, \vec{x}_t)
%\end{equation}
donde $Tanh$ es la función de activación, que está siendo aplicada a cada elemento del vector resultante de la multiplicación y suma de las matrices de peso $W$ por los vectores $\vec{x}_t$ y $\vec{h}_{t-1}$ para cada tiempo $t$. La salida, por su parte, está dada por:
\begin{equation}
  \vec{y}_t = W_{hy}\vec{h}_t
\end{equation}

\begin{figure}[!htbp]
  \centering
  \includegraphics[width=0.9\textwidth]{/home/jessica/Tesis/img/tesis/RNN.png}
  \caption{Diagrama de una \acs{RNN} y su representación en capas de tiempo.}
  \label{fig:RNN}
\end{figure}

En la \autoref{fig:RNN}, a la derecha se muestra la representación de la \acs{RNN} en capas para cada tiempo, en donde es importante notar que las matrices de peso $W$ son las mismas en cada tiempo $t$, de manera que ecuación de recurrencia \autoref{eq:hidden} aplica la misma función.
\\

Uno de los principales problemas de las \acp{RNN} en la práctica es en el entrenamiento, pues el gradiente tiende a desvanecerse o incrementar excesivamente, debido a la inestabilidad que produce la multiplicación de forma recursiva de las matrices de peso $W$, problema que puede incrementarse cuanto más grande sea la secuencia de tiempo que procesa la red y cuantas más capas ocultas tenga.
\\
Una alternativa que previene este problema es el cambio de la relación de recurrencia para el estado oculto $\vec{h}_t$ (\autoref{eq:hidden}) mediante el uso de la \emph{memoria a largo plazo} de una red \acs{LSTM}, que se abordará en la siguiente sección. \cite{Nielsen:2018}

\section{Redes Long-Short Term Memory}\label{sec:lstm}

Las redes Long-Short Term Memory son un tipo de \acs{RNN} diseñadas para mitigar los problemas con el gradiente que estas tienen en el entrenamiento. La manera en la que realizan esto es cambiando las condiciones de recurrencia de cómo el estado oculto $\vec{h}_t$ es propagado en el tiempo, para ello, se introduce un nuevo vector llamado \textbf{celda de estado}: $\vec{c}_{t}$ de la misma dimensión de $\vec{h}_t$, que puede interpretarse como una \emph{memoria a largo plazo} que retiene parte de la información de las celdas de estados de tiempos pasados. \cite{Nielsen:2018}

\begin{figure}[!htbp]
  \centering
  \includegraphics[width=0.9\textwidth]{/home/jessica/Tesis/img/tesis/LSTM_layerS.drawio.png}
  \caption{Diagrama de una \acs{LSTM} representada en capas de tiempo.}
  \label{fig:LSTMlayerS}
\end{figure}

El diagrama de la \autoref{fig:LSTMlayerS} muestra la estructura de una \acs{LSTM} desplegada en capas de tiempo, su forma en cadena es igual a la de las \acp{RNN} mencionadas en la sección anterior, sin embargo, cada bloque de memoria (recuadros rosas) tiene una arquitectura particular más compleja que la de una \acs{RNN} común.

\subsection{Arquitectura}\label{sec:LSTM_Arch}
La \autoref{fig:LSTMlayer} muestra un bloque de memoria de una \acs{LSTM}, en donde se observa cómo interactúan los vectores de estado oculto y de celda de estado pasados: $\vec{h}_{t-1}$ y $\vec{c}_{t-1}$ con funciones llamadas compuertas: $\vec{f}_t$, $\vec{i}_t$, $\vec{o}_t$ y operaciones básicas como suma y multiplicación, para producir nuevos vectores $\vec{h}_{t}$ y $\vec{c}_{t}$. Al tiempo $t=0$, los vectores de estado oculto y de celda de estado comienzan inicializados con un valor aleatorio.

\begin{figure}[!htbp]
  \centering
  \includegraphics[width=0.7\textwidth]{/home/jessica/Tesis/img/tesis/LSTM_layer.png}
  \caption{Diagrama de un Bloque de Memoria de una \acs{LSTM}.}
  \label{fig:LSTMlayer}
\end{figure}

Una compuerta llamada \textbf{forget} $\vec{f}$, decide qué información proveniente del tiempo anterior: $t-1$ se mantiene y cual no, tomando en consideración también la entrada $\vec{x}_t$, aplicando la función $\sigma$ (es decir, $\vec{f}$ es una capa \emph{Sigmoid}):

\begin{equation}\label{eq:ft}
\vec{f}_t = \sigma(W_{xf}\vec{x}_t + W_{hf}\vec{h}_{t-1} + b_f)
\end{equation}

En donde $W$ y $b$ con sus correspondientes subíndices representan las matrices de peso y vectores de sesgo correspondientes a cada capa oculta. \\
Posteriormente, para decidir qué nueva información será incluida a la celda de estado se calculan:

\begin{equation}\label{eq:it}
\vec{i}_t = \sigma(W_{xi}\vec{x}_t + W_{hi}\vec{h}_{t-1} + b_i)
\end{equation}
\begin{equation}
  \label{eq:gt}
\vec{g}_t = Tanh(W_{xg}\vec{x}_t + W_{hg}\vec{h}_{t-1} + b_g)  
\end{equation}

en donde $\vec{i}$ es una capa \emph{Sigmoid}, llamada compuerta \textbf{input}, y $\vec{g}$ es una capa \emph{Tanh} que genera un vector con valores nuevos que son candidatos para ser agregados a la nueva celda de estado $\vec{c}_t$.
\\
La actualización de la celda de estado se da como:

\begin{equation}\label{eq:ct}
\vec{c}_t = \vec{f}_t \ast \vec{c}_{t-1} + \vec{i}_t \ast \vec{g}_t
\end{equation}

en donde $\ast$ representa el \emph{producto Hadamard}, en donde la multiplicación de los vectores se realiza elemento por elemento. Finalmente, la salida esta dada por:

\begin{equation}\label{eq:ot}
\vec{o}_t = \sigma(W_{xo}\vec{x}_t + W_{ho}\vec{h}_{t-1} + b_o)
\end{equation}

\begin{equation}\label{eq:ht}
\vec{h}_t = \vec{o}_t\ast Tanh(\vec{c}_t)
\end{equation}

en donde $\vec{o}$ es una capa \emph{Sigmoid} llamada compuerta \textbf{output}, y el vector $\vec{h}_t$ es el estado oculto al tiempo $t$, que servirá como entrada, junto con el vector de celda de estado $\vec{c}_t$ para el bloque de memoria del tiempo $t+1$, como se muestra en la \autoref{fig:LSTMlayerS}.

% ************************************************************
% ************************************************************
% PROYECTO
% ************************************************************
% ************************************************************

\section{Propagación temporal de Funciones de Onda con LSTM}\label{sec:Project}

En esta sección se implementa una red \acs{LSTM} para predecir la evolución temporal de un paquete de onda a un tiempo t $\psi(r,t)$ bajo un potencial dependiente del tiempo $V(r,t)$ después de un intervalo de tiempo $\Delta t$: $\psi(r,t+\Delta t)$. Tomando como referencia un trabajo previo \cite{Main:2021} en donde se implementó un modelo de perceptrón multicapa.

\subsection{Obtención de Datos}

El conjunto de datos de entrenamiento utilizado se obtuvo generando 4700 trayectorias de $200\,fs$, cada una con un $\Delta t = 1\,fs$. El diagrama de la \autoref{fig:DiagTraj} muestra una trayectoria, en donde $\psi(r,t_l)r$ con $l=0,\dots 200$ representa la parte real del paquete de onda, $\psi(r,t_l)i$ la parte imaginaria y $V(r,t_l)$ el potencial.

\begin{figure}[!htbp]
  \centering
  \includegraphics[width=0.7\textwidth]{/home/jessica/Tesis/img/tesis/DiagTrayectoria.png}
  \caption{Diagrama de una trayectoria generada.\\Para cada tiempo $t_i$ la onda $\psi(r,t_i)$ contiene el valor de la onda en los $N=32$ puntos en el grid del espacio de posiciones en $r$.}
  \label{fig:DiagTraj}
\end{figure}

Para cada trayectoria, los paquetes de onda iniciales, es decir al tiempo $t=0$, se definieron como ondas Gaussianas: (\autoref{fig:Gauss})
\begin{figure}[!htbp]
  \centering
  \includegraphics[width=1\textwidth]{/home/jessica/Tesis/img/tesis/DataWave.png}
  \caption{Parte real y compleja de un paquete de onda inicial $\psi(r,t=0)$.}
  \label{fig:Gauss}
\end{figure}

\begin{equation}
  \label{eq:gaussian}
  \psi(r,0) = C_i\cdot \frac{1}{\sigma\sqrt{2\pi}}\exp{\frac{-(r-\mu)^2}{2\sigma^2}}
\end{equation}
en donde $\mu$ y $\sigma$ son valores aleatorios elegidos de una distribución uniforme de $(-0.5,0.5)\mathring{A}$ y $(0.1,0.3)\mathring{A}$ respectivamente. $C_i$ es un número complejo aleatorio elegido de tal manera que la onda esté normalizada, es decir:
$$\bra{\psi(r,0)}\ket{\psi(r,0)} = 1$$

Para propagar la onda se utilizó el método \acs{DVR} revisado en la sección \autoref{sec:DVRapp} con un grid de $N=32$ puntos, con $a=r_0=-1.5\mathring{A}$ y $b=r_{N-1}=1.5\mathring{A}$ (\autoref{eq:grid}).
\\
El modelo de potencial $V(r,t)$ utilizado fue el mismo que se usó en la sección \autoref{sec:ProtonTransfer}. Para cada trayectoria se eligieron valores de parámetros para el modelo de potencial aleatorios entre rangos especificados en la \autoref{tab:RangeValuesPot}. En la \autoref{fig:Pote} se muestra un ejemplo de un potencial generado.
\begin{figure}[!htbp]
  \centering
  \includegraphics[width=1\textwidth]{/home/jessica/Tesis/img/tesis/DataPot.png}
  \caption{Potencial inicial al tiempo $t=0$}
  \label{fig:Pote}
\end{figure}

En el repositorio \href{https://github.com/Jessi-MM/PropagatorLearning/tree/main/Data_Gaussian}{\faGithub Trayectorias} se encuentran los datos generados. Cada carpeta contiene los directorios y archivos siguientes:

 \begin{forest}
      for tree={
        font=\ttfamily,
        grow'=0,
        math content,
        align=center,
        child anchor=west,
        parent anchor=south,
        anchor=west,
        calign=first,
        inner xsep=7pt,
        edge path={
          \noexpand\path [draw, \forestoption{edge}]
          (!u.south west) +(7.5pt,0) |- (.child anchor) pic {folder} \forestoption{edge label};
        },
        % style for your file node 
        file/.style={edge path={\noexpand\path [draw, \forestoption{edge}]
          (!u.south west) +(7.5pt,0) |- (.child anchor) \forestoption{edge label};},
          inner xsep=2pt,font=\small\ttfamily
                     },
        before typesetting nodes={
          if n=1
            {insert before={[,phantom]}}
            {}
        },
        fit=band,
        before computing xy={l=15pt},
      }  
    [Trayectorias
      [data0
      [Potential
      [0-Potential.npy\\$\vdots$,file]
      [200-Potential.npy,file
      ]
        ]
        [Wavepacket
        [0-Wavepacket.npy\\$\vdots$,file
        ]
        [200-Wavepacket.npy,file
        ]
        ]
      ]
    ]
 \end{forest}

 En donde \texttt{data0} corresponde a la trayectoria $0$. El archivo \emph{0-Potential.npy} es un arreglo de $N=32$ entradas que corresponde al valor del potencial al tiempo $t=0\,fs$, el archivo \emph{0-Wavepacket.npy} es un arreglo de $N=32$ entradas complejas que corresponden al paquete de onda al tiempo $t=0\,fs$; así respectivamente para cada tiempo hasta $t=200\,fs$. Además de los datos, se encuentra un archivo de texto: \emph{ValuesPotential.txt}, en donde se registran los valores de los parámetros para generar el potencial, es decir, los valores exactos elegidos de la \autoref{tab:RangeValuesPot}.
 \\
 
 El código para generar las trayectorias se encuentra en el repositorio: \href{https://github.com/Jessi-MM/PropagatorLearning/blob/main/src/Proton_Transfer_DataGenerate.ipynb}{\faGithub Generación de datos}, que contiene dos clases: \emph{Potential\_System}, que contiene las ecuaciones del modelo del potencial y la clase \emph{ProtonTransfer}, con la que se generan las trayectorias. A continuación se muestra un ejemplo para generar una trayectoria, con la información que muestra el diagrama de la \autoref{fig:DiagTraj}:

 \begin{lstlisting}[language=Python, morekeywords={ProtonTransfer, True, False}]
   trayectoria0 = ProtonTransfer(n=32, a=-1.5, b=1.5, time=True, var_random=True, save_dir='data0')
   trayectoria0.vector_potential(t=200,step=1)
   trayectoria0.evolution_wp(t=200, step=1, gaussiana=True)\end{lstlisting}

En donde se utilizan las siguientes variables:
\begin{itemize}[label=\textcolor{CTtitle}{\textbullet}]
\item n: Número de puntos en el grid
\item a: Punto inicial del grid $[\mathring{A}]$
\item b: Punto final del grid $[\mathring{A}]$
\item time: True o False. Determina si se utiliza un potencial dependiente del tiempo: True, o independiente del tiempo: False  
\item var\_random: True o False. True inicia las variables de manera aleatoria para el potencial del sistema. False solicita al usuario cada variable. \autoref{tab:RangeValuesPot}
\item save\_dir: Nombre del directorio donde se guardarán los datos del potencial y la evolución de onda.
\item t: Tiempo total de la trayectoria $[fs]$
\item step: $\Delta t$ para la evolución temporal de la onda $[fs]$
\item gaussiana: True o False. True genera una onda inicial Gaussiana (\autoref{eq:gaussian}), False solicita $k$ para generar la onda inicial como una suma de eigenfunciones (\autoref{eq:psi_0}) 
\end{itemize}

\subsection{Procesamiento de Datos: Visualización y Forma}

Para el entrenamiento de la red, las trayectorias deben estar conformados por entradas $X$ y etiquetas o salidas $y$, para este problema, cada entrada está conformada por la parte real y compleja de un paquete onda y el potencial al tiempo $t$, es decir un vector de la forma:

$$(\psi_{real}(r,t), \psi_{imag}(r,t),V(r,t))$$

para $t \in \{0,1,\dots,199\}$ de dimensión:

$$(32+32+32,200)=(96,200)$$

mientras que la salida es el paquete de onda propagado al tiempo $t+1$:

$$(\psi_{real}(r,t+1), \psi_{imag}(r,t+1))$$

para $t \in \{0,1,\dots,199\}$ de dimensión:

$$(32+32,200)=(64,200)$$

La \autoref{fig:datavis} muestra cada componente de los vectores de entrada y salida en su representación del espacio de posiciones $r$ a un tiempo $t$, en donde las funciones de onda $\psi$ están escaladas por un factor de 20 para poder visualizar el potencial en la misma gráfica.

\begin{figure}[!htbp]
  \centering
  \includegraphics[width=1\textwidth]{/home/jessica/Tesis/img/tesis/dataInputOutput.png}
  \caption{Visualización gráfica: Entrada $X$ y etiqueta $y$ a un tiempo $t$.}
  \label{fig:datavis}
\end{figure}

El código para la preparación de los datos se encuentra en la \href{https://github.com/Jessi-MM/PropagatorLearning/tree/main/Data_Gaussian}{\faGithub Notebook principal} en la clase \texttt{Propagator\_Dataset}.\\
La división de los datos se muestra en la \autoref{tab:SplitData}.

\begin{table}[ht]
  \myfloatalign
  \begin{tabularx}{0.7\textwidth}{XXXX} \toprule
   \tableheadline{Entrenamiento} & \tableheadline{Validación} & \tableheadline{Test} & \tableheadline{Total} \\ \midrule
   5600          &  1600  & 800  & 8000  \\ \midrule
   70\%          &  20\% & 10\% & 100\% \\
    \bottomrule
  \end{tabularx}
  \caption{División de datos}
  \label{tab:SplitData}
\end{table}

\subsection{Modelo LSTM: Arquitectura y Entrenamiento}

Para el modelo de la red y su entrenamiento se utilizó PyTorch versión 1.9.0+cu102 en una computadora personal. El resumen del modelo se muestra en la \autoref{tab:model}.

\begin{table}[ht]
  \myfloatalign
  \begin{tabularx}{\textwidth}{XXX} \toprule
   \tableheadline{Capas} & \tableheadline{Nodos} & \tableheadline{Entrada/Salida} \\ \midrule
   LSTM          &  1024  & (In: 96, Ou:1024)  \\ \midrule
   LSTM          &  1024  & (In: 1024, Ou:1024)  \\ \midrule
   Lineal        &  64    & (In: 1024, Ou:64) \\
   \bottomrule
   Loss Function:      & Mean Square Error \\
   Optimizer:          & AdamW &  weight decay = 0.01 \\
   Batch size:         & 10 \\
   Learning rate:      & 0.0001 \\
   Epochs:             & 290 \\
   \bottomrule
   
  \end{tabularx}
  \caption{Resumen del modelo}
  \label{tab:model}
\end{table}

En donde los parámetros utilizados fueron elegidos después de varios intentos para hallar los que minimizaran la función de pérdida o loss function más rápido, es decir en un menor número de épocas. Las funciones de activación en las capas \acs{LSTM} son las mencionadas en la sección \autoref{sec:lstm}, mientras que la última capa corresponde a una capa lineal debido a que la salida $y$ corresponde a valores reales. El algoritmo de optimización\footnote{Detalles en la sección \autoref{sec:TrainNN}} utiliza una técnica de regularización para prevenir el sobre-entrenamiento, que consiste en agregar una pequeña penalización a la pérdida, que es la norma $L_2$ de todos los pesos entrenables del modelo: \cite{https://doi.org/10.48550/arxiv.1711.05101}

\[ loss = loss + \text{weight decay}*L_2\]

En la \autoref{fig:Model} se muestra un diagrama del modelo de red representado en capas de tiempo (sección \autoref{sec:lstm}), con las entradas y salidas correspondes.

\begin{figure}[!htbp]
  \centering
  \includegraphics[width=1\textwidth]{/home/jessica/Tesis/img/tesis/model/modelDiag2.png}
  \caption{Modelo LSTM.}
  \label{fig:Model}
\end{figure}

Para calcular la precisión del modelo se utilizó la \autoref{eq:S}:

\begin{equation}
  \label{eq:S}
  S = \bra{\psi_{LSTM}}\ket{\psi_{True}} = |S|\exp{i\theta}
\end{equation}

que compara dos funciones de onda, la predicha por el modelo $\psi_{LSTM}$ respecto a la esperada $\psi_{True}$, cuando se trata de la misma función de onda, dado que están normalizadas, se sigue que:
\begin{equation*}
  \bra{\psi}\ket{\psi} = \int_{a}^{b}\psi(x) \psi(x)^\dag dx = \int_{a}^{b}|\psi(x)| dx = 1 = |S|\exp{i\theta}
\end{equation*}

$\iff |S|=1$ y $\theta =0$. Es decir, que la función de onda $\psi_{LSTM}$ predicha por el modelo es más parecida a la esperada $\psi_{True}$ cuanto más se acerca $|S|$ a $1$ y $\theta$ a $0$. \\

Al finalizar cada época en el entrenamiento se calculó la función de precisión para cada onda en cada trayectoria en el conjunto de validación y se promediaron los valores correspondientes a $|S|$ y $\theta$. 

\subsection{Resultados y Análisis: Precisión y Predicciones}\label{sec:Resultados}

En la \autoref{fig:S-phase} se muestran los valores promedio obtenidos de la magnitud absoluta $|S|$ y la fase $\theta$ a través de las épocas.

\begin{figure}[!htbp]
  \centering
  \includegraphics[width=1\textwidth]{/home/jessica/Tesis/img/tesis/S-plot.png}
  \caption{Valores promedio de $|S|$ y $\theta$ en el conjunto de validación por cada época.}
  \label{fig:S-phase}
\end{figure}

Al finalizar el entrenamiento, sobre el conjunto de testeo los resultados de las variables se reportan en la \autoref{tab:res-S}.

\begin{table}[ht]
  \myfloatalign
  \begin{tabularx}{0.3\textwidth}{XX} \toprule
   \tableheadline{$|S|$} & \tableheadline{$\theta$} \\ \midrule
   0.946          &  0.0001 \\
   \bottomrule  
  \end{tabularx}
  \caption{Valores promedio de $|S|$ y $\theta$ en el conjunto de testeo}
  \label{tab:res-S}
\end{table}




\begin{figure}[!htbp]
  \centering
  \includegraphics[width=1\textwidth]{/home/jessica/Tesis/img/tesis/model/1step.png}
  \caption{Predicción 1}
  \label{fig:1step}
\end{figure}

\begin{figure}[!htbp]
  \centering
  \includegraphics[width=1\textwidth]{/home/jessica/Tesis/img/tesis/model/1step1.png}
  \caption{Predicción 2}
  \label{fig:1step1}
\end{figure}

\begin{figure}[!htbp]
  \centering
  \includegraphics[width=1\textwidth]{/home/jessica/Tesis/img/tesis/model/trajDens7.png}
  \caption{Trayectoria 1}
  \label{fig:trajec1}
\end{figure}

\begin{figure}[!htbp]
  \centering
  \includegraphics[width=1\textwidth]{/home/jessica/Tesis/img/tesis/model/trajDens1.png}
  \caption{Trayectoria 2}
  \label{fig:trajec2}
\end{figure}


\subsection{Aumento de resolución con 2 LSTM}



\subsection{Conclusión}


