\message{ !name(cap05.tex)}
\message{ !name(cap05.tex) !offset(-2) }
%*************************************************
\chapter{Redes Neuronales Artificales}\label{ch:NNBasics}
% ************************************************
En el capítulo anterior se abordaron conceptos básicos del aprendizaje automático, así como dos de sus principales enfoques: aprendizaje supervisado y no supervisado. Las Redes Neuronales Artificiales son comúnmente un método de aprendizaje supervisado\footnote{Las Redes Neuronales Artificales son algoritmos que también se pueden aplicar en métodos de aprendizaje no supervisado.}, que han tenido gran impulso en los últimos años debido al incremento de datos, su fácil acceso, y al crecimiento en poder computacional, que ha permitido el desarrollo de métodos de 





\section{Arquitectura básica}


\begin{figure}[hb]
  \centering
  \begin{tikzpicture}[x=1.5cm, y=1.5cm]
  % Input nodes
  \foreach \i in {1,2,3}
    \node[circle, draw, fill=CTtitle!30] (input\i) at (0,\i) {$x_{\i}$};
  % Summation node
  \node[circle, draw, fill=CTtitle!60, minimum size=1.5cm] (sum) at (2,2) {$\Sigma$};
  % Activation node
  %\node[circle, draw, fill=CTtitle!50] (activation) at (6,2) {$y$};
  % Output node
  \node[circle, draw, fill=CTtitle!90] (output) at (4,2) {$\hat{y}$};
  % Connect the nodes and add weights
  \foreach \i in {1,2,3}
  \draw[->] (input\i) -- node[midway, above] {$w_{\i}$} (sum);
  %\draw[->] (sum) -- (activation);
  \draw[->] (sum) -- (output);

  \node[above, yshift=2.5cm] at (0,2) {Entrada};
  \node[above, yshift=2.5cm] at (2,2) {Neurona};
  \node[above, yshift=2.5cm] at (4,2) {Salida};
  
\end{tikzpicture}

\caption{Modelo simple de un Perceptrón}
\label{fig:perceptron}
\end{figure}






\begin{figure}
    \centering
    \begin{tikzpicture}[scale=1.5]
    
    % Input layer
    \foreach \i in {1,...,4}
        \node[circle, draw=black, fill=CTtitle!30] (I-\i) at (0,\i-2) {$x_{\i}$};
    
    % Hidden layer 1
    \foreach \i in {1,...,3}
        \node[circle, draw=black, fill=CTtitle!60] (H1-\i) at (2,\i-1.5) {};
    
    % Hidden layer 2
    \foreach \i in {1,...,3}
        \node[circle, draw=black, fill=CTtitle!60] (H2-\i) at (4,\i-1.5) {};
        
    % Output layer
    \foreach \i in {1,...,2}
        \node[circle, draw=black, fill=CTtitle!90] (O-\i) at (6,\i-1) {$y_{\i}$};
    
    % Connections
    \foreach \i in {1,...,4}
        \foreach \j in {1,...,3}
            \draw[->] (I-\i) -- (H1-\j);
            
    \foreach \i in {1,...,3}
        \foreach \j in {1,...,3}
            \draw[->] (H1-\i) -- (H2-\j);
            
    \foreach \i in {1,...,3}
        \foreach \j in {1,...,2}
            \draw[->] (H2-\i) -- (O-\j);
    
    % Labels
    \node[above, yshift=0.5cm] at (0,2) {Capa de Entrada};
    \node[above, yshift=0.5cm] at (2,2) {Capa oculta 1};
    \node[above, yshift=0.5cm] at (4,2) {Capa oculta 2};
    \node[above, yshift=0.5cm] at (6,2) {Capa de Salida};
    
    \end{tikzpicture}
    \caption{Arquitectura básica de una Red Neuronal Artificial}
    \label{fig:nn}
\end{figure}

\section{Arquitectura básica}
%---- sec 1.2.1 NN & Deep Learning book
% Training set 
% input
% hidden layer
% output

\section{Función de Activación y Función de Pérdida}
\section{Salida y su Función de Pérdida}

\section{Entrenamiento de una Red Neuronal}
% backpropagation

\section{Multilayer Perceptron, LSTM ? or other NN maybe}
\message{ !name(cap05.tex) !offset(-106) }
